{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a051c5c-2e88-41bd-9911-36ee82fcae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b0b954-a145-42b3-8a82-f1e419ab74c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftTopK(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def _solve(s, t, a, b, e):\n",
    "        z = torch.abs(e) + torch.sqrt(e**2 + a * b * torch.exp(s - t))\n",
    "        ab = torch.where(e > 0, a, b)\n",
    "\n",
    "        return torch.where(\n",
    "            e > 0, t + torch.log(z) - torch.log(ab), s - torch.log(z) + torch.log(ab)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, r, k, alpha, descending=False):\n",
    "        assert r.shape[0] == k.shape[0], \"k must have same batch size as r\"\n",
    "\n",
    "        batch_size, num_dim = r.shape\n",
    "        x = torch.empty_like(r, requires_grad=False)\n",
    "\n",
    "        def finding_b():\n",
    "            scaled = torch.sort(r, dim=1)[0]\n",
    "            scaled.div_(alpha)\n",
    "\n",
    "            eB = torch.logcumsumexp(scaled, dim=1)\n",
    "            eB.sub_(scaled).exp_()\n",
    "\n",
    "            torch.neg(scaled, out=x)\n",
    "            eA = torch.flip(x, dims=(1,))\n",
    "            torch.logcumsumexp(eA, dim=1, out=x)\n",
    "            idx = torch.arange(start=num_dim - 1, end=-1, step=-1, device=x.device)\n",
    "            torch.index_select(x, 1, idx, out=eA)\n",
    "            eA.add_(scaled).exp_()\n",
    "\n",
    "            row = torch.arange(1, 2 * num_dim + 1, 2, device=r.device)\n",
    "\n",
    "            torch.add(torch.add(eA, eB, alpha=-1, out=x), row.view(1, -1), out=x)\n",
    "\n",
    "            w = (k if descending else num_dim - k).unsqueeze(1)\n",
    "            i = torch.searchsorted(x, 2 * w)\n",
    "            m = torch.clamp(i - 1, 0, num_dim - 1)\n",
    "            n = torch.clamp(i, 0, num_dim - 1)\n",
    "\n",
    "            b = SoftTopK._solve(\n",
    "                scaled.gather(1, m),\n",
    "                scaled.gather(1, n),\n",
    "                torch.where(i < num_dim, eA.gather(1, n), 0),\n",
    "                torch.where(i > 0, eB.gather(1, m), 0),\n",
    "                w - i,\n",
    "            )\n",
    "            return b\n",
    "\n",
    "        b = finding_b()\n",
    "\n",
    "        sign = -1 if descending else 1\n",
    "        torch.div(r, alpha * sign, out=x)\n",
    "        x.sub_(sign * b)\n",
    "\n",
    "        sign_x = x > 0\n",
    "        p = torch.abs(x)\n",
    "        p.neg_().exp_().mul_(0.5)\n",
    "\n",
    "        inv_alpha = -sign / alpha\n",
    "        S = torch.sum(p, dim=1, keepdim=True).mul_(inv_alpha)\n",
    "\n",
    "        torch.where(sign_x, 1 - p, p, out=p)\n",
    "\n",
    "        ctx.save_for_backward(r, x, S)\n",
    "        ctx.alpha = alpha\n",
    "        return p\n",
    "\n",
    "    # @staticmethod\n",
    "    # def backward(ctx, grad_output):\n",
    "    #     r, x, S = ctx.saved_tensors\n",
    "    #     alpha = ctx.alpha\n",
    "\n",
    "    #     q = torch.softmax(-torch.abs(x), dim=1)\n",
    "    #     qgrad = q * grad_output\n",
    "\n",
    "    #     # Gradients\n",
    "    #     grad_k=qgrad.sum(dim=1)\n",
    "    #     grad_r = S * q * (grad_k.unsqueeze(1)-grad_output)\n",
    "    #     grad_alpha = (S / alpha * qgrad * (r - (q * r).sum(dim=1, keepdim=True))).sum()\n",
    "    #     return grad_r, grad_k, grad_alpha, None\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        r, x, S = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "\n",
    "        x.abs_().neg_()\n",
    "        q = torch.softmax(x, dim=1)\n",
    "\n",
    "        torch.mul(q, grad_output, out=x)\n",
    "        grad_k = x.sum(dim=1, keepdim=True)\n",
    "\n",
    "        grad_r = grad_k - grad_output\n",
    "        grad_r.mul_(q).mul_(S)\n",
    "\n",
    "        q.mul_(r)\n",
    "        x.mul_(S / alpha)  # grad_alpha = (S / alpha) * x\n",
    "        r.sub_(q.sum(dim=1, keepdim=True))\n",
    "        x.mul_(r)  # grad_alpha.mul_(r)\n",
    "        grad_alpha = x.sum()  # grad_alpha = grad_alpha.sum()\n",
    "        return grad_r, grad_k.squeeze(1), grad_alpha, None\n",
    "\n",
    "\n",
    "def soft_top_k(r, k, alpha, descending=False):\n",
    "    return SoftTopK.apply(r, k, alpha, descending)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1755019-ba2f-42d0-88be-3ab1ad713742",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2ae88-71f2-496f-af69-e0d7de2a46ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value(x, v, text):\n",
    "    assert x.shape == v.shape, f\"Shape mismatch: {x.shape} vs {v.shape}\"\n",
    "\n",
    "    def fun():\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return torch.allclose, torch.linalg.norm\n",
    "        else:\n",
    "            return np.allclose, np.linalg.norm\n",
    "\n",
    "    function, dist = fun()\n",
    "    check = None\n",
    "    for tol_exp in range(-15, 0):\n",
    "        if function(x, v, rtol=1e-05, atol=10**tol_exp):\n",
    "            check = f\"Error within atol=1e{tol_exp}\"\n",
    "            break\n",
    "    if check:\n",
    "        print(f\"✅ - {text} ({check})\")\n",
    "    else:\n",
    "        print(f\"❌ - {text} [dist: {dist(x - v):.4f}]\")\n",
    "        print(f\"Expected: {v}\")\n",
    "        print(f\"Got: {x}\")\n",
    "\n",
    "\n",
    "def print_time_stats(times, name):\n",
    "    if not times:\n",
    "        return\n",
    "    avg = sum(times) / len(times)\n",
    "    min_t = min(times)\n",
    "    max_t = max(times)\n",
    "    print(f\"\\n{name} time stats (seconds):\")\n",
    "    print(f\"\\033[0;1;35m  Average: {avg:.4f}\\033[0m\")\n",
    "    print(f\"  Min:     {min_t:.4f}\")\n",
    "    print(f\"  Max:     {max_t:.4f}\")\n",
    "    print(f\"  All times: {[f'{t:.4f}' for t in times]}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# ==============  Parameters  =================\n",
    "use_gpu = False\n",
    "use_gpu = True\n",
    "\n",
    "descending = False\n",
    "# descending = True\n",
    "\n",
    "h = 1e-5\n",
    "\n",
    "bs = 3\n",
    "n = 500\n",
    "# =============================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "print(f\"{device=}\\n\")\n",
    "\n",
    "factory_kwargs = {\"device\": device, \"requires_grad\": True}\n",
    "\n",
    "for i in range(3):\n",
    "    alpha = torch.tensor(np.random.rand(), **factory_kwargs)\n",
    "\n",
    "    r = torch.randn(bs, n, **factory_kwargs)\n",
    "    k = torch.tensor(np.random.rand(bs) * n, **{**factory_kwargs, \"dtype\": r.dtype})\n",
    "\n",
    "    print(f\"bs={bs}, n={n}, alpha={alpha.item()}\")\n",
    "    assert (\n",
    "        alpha.dtype == k.dtype == r.dtype\n",
    "    ), f\"You have different types of tensors: {alpha.dtype=}, {k.dtype=}, {r.dtype=}\"\n",
    "\n",
    "    # For Backward computation\n",
    "    v = torch.randn_like(r)\n",
    "\n",
    "    # Forward pass\n",
    "    start_forward = time.perf_counter()\n",
    "    prob = soft_top_k(r, k, alpha, descending)\n",
    "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "    forward_time = time.perf_counter() - start_forward\n",
    "    print(f\"\\033[0;32mForward pass time: {forward_time:.4g} s\\033[0m\")\n",
    "\n",
    "    # Test sum\n",
    "    test_sum = prob.sum(dim=-1)\n",
    "    check_value(test_sum, k, \"test sum\")\n",
    "\n",
    "    # ======================================================\n",
    "    print(\"=\" * 10, \"Gradients\", \"=\" * 10, sep=\"   \")\n",
    "\n",
    "    # Backward pass\n",
    "    start_backward = time.perf_counter()\n",
    "    r.grad = None  # Clear gradients\n",
    "    k.grad = None\n",
    "    alpha.grad = None\n",
    "    prob.backward(v)\n",
    "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "    backward_time = time.perf_counter() - start_backward\n",
    "    print(f\"\\033[0;34mBackward pass time: {backward_time:.4g} s\\033[0m\")\n",
    "    print(f\"\\033[0;33mTotal time: {forward_time + backward_time:.4g} s\\033[0m\")\n",
    "\n",
    "    # try:\n",
    "    #     torch.autograd.gradcheck(\n",
    "    #         lambda r: soft_top_k(r, k, alpha, descending),\n",
    "    #         r,\n",
    "    #         eps=1e-6,\n",
    "    #         atol=1e-5,\n",
    "    #         rtol=1e-3\n",
    "    #     )\n",
    "    #     print(\"✅ r gradient passed\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"❌ r gradient failed: {str(e)}\")\n",
    "\n",
    "    # try:\n",
    "    #     torch.autograd.gradcheck(\n",
    "    #         lambda k: soft_top_k(r, k, alpha, descending).sum(),\n",
    "    #         k,\n",
    "    #         eps=1e-6,\n",
    "    #         atol=1e-5,\n",
    "    #         rtol=1e-3\n",
    "    #     )\n",
    "    #     print(\"✅ k gradient passed\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"❌ k gradient failed: {str(e)}\")\n",
    "\n",
    "    # try:\n",
    "    #     torch.autograd.gradcheck(\n",
    "    #         lambda a: soft_top_k(r, k, a, descending),\n",
    "    #         alpha,\n",
    "    #         eps=1e-6,\n",
    "    #         atol=1e-5,\n",
    "    #         rtol=1e-3\n",
    "    #     )\n",
    "    #     print(\"✅ alpha gradient passed\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"❌ alpha gradient failed: {str(e)}\")\n",
    "\n",
    "    numerical_derivative = (\n",
    "        soft_top_k(r + h * v, k, alpha, descending)\n",
    "        - soft_top_k(r - h * v, k, alpha, descending)\n",
    "    ) / (2 * h)\n",
    "    check_value(r.grad, numerical_derivative, \"grad r\")\n",
    "\n",
    "    numerical_k_grad = (\n",
    "        torch.mul(\n",
    "            v,\n",
    "            soft_top_k(r, k + h, alpha, descending)\n",
    "            - soft_top_k(r, k - h, alpha, descending),\n",
    "        )\n",
    "        / (2 * h)\n",
    "    ).sum(1)\n",
    "    check_value(k.grad, numerical_k_grad, \"grad k\")\n",
    "\n",
    "    numerical_alpha_grad = torch.mul(\n",
    "        v,\n",
    "        soft_top_k(r, k, alpha + h, descending)\n",
    "        - soft_top_k(r, k, alpha - h, descending),\n",
    "    ) / (2 * h)\n",
    "    check_value(alpha.grad, numerical_alpha_grad.sum(), \"grad alpha\")\n",
    "    print()"
   ],
   "id": "7476f0d1fb8c64c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "73b2a48a16660b6e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
