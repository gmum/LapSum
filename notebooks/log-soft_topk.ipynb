{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Implementation of function log soft_top_k\n",
    "\n",
    "In some cases, especially for CE loss, we will need rather $\\log p$ instead of $p$.\n",
    "\n",
    "*Value of $\\log p$*\n",
    "We have:\n",
    "$$\n",
    "\\log p_i = g(\\frac{b - r_i}{\\alpha}),\n",
    "$$\n",
    "where:\n",
    "$$\n",
    "g(x) =\n",
    "\\begin{cases}\n",
    "\\log(1 - 0.5 \\exp(-|x|)) & \\text{for } x \\geq 0, \\\\\n",
    "-\\log 2 + x & \\text{for } x < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To calculate derivatives, we need the auxiliary function:\n",
    "$$\n",
    "h(x) =\n",
    "\\frac{1}{\\max(x,0.5)} - 1\n",
    "$$\n",
    "\n",
    "*Derivative with respect to $w$*\n",
    "We have\n",
    "   $$\n",
    "   \\frac{\\partial u_i}{\\partial w} = \\frac{1}{p_i}\\cdot \\frac{\\partial p_i}{\\partial w}=\\frac{q_i}{p_i}.\n",
    "   $$\n",
    "  Thus\n",
    "  $$\n",
    "    \\frac{\\partial u}{\\partial w} =q/p= h(p)/(S\\alpha). \n",
    "  $$\n",
    "\n",
    "*Derivative of $u$ with respect to $\\alpha$* \n",
    "We have:\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial \\alpha} =\n",
    "\\frac{1}{p} \\odot \\frac{\\partial p}{\\partial \\alpha}.\n",
    "$$\n",
    "Substituting:\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial \\alpha} =\n",
    "\\frac{1}{p} \\odot \\frac{1}{\\alpha}(s \\odot r-\\langle q,r \\rangle s).\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial \\alpha} =\n",
    "\\frac{1}{\\alpha^2}(h(p) \\odot r - \\langle q, r \\rangle h(p)).\n",
    "$$\n",
    "\n",
    "*Derivative of $u$ with respect to $r$*\n",
    "We have\n",
    "$$\n",
    "Du=\\mathrm{diag}\\left(\\frac{1}{p}\\right)\n",
    "$$\n",
    "$$\n",
    "Du=\\mathrm{diag}\\left(\\frac{1}{p}\\right)(sq^T-\\mathrm{diag}(s))\n",
    "=\n",
    "\\frac{1}{\\alpha}\\left(h(p) q^T-\\mathrm{diag}(h(p))\\right)\n",
    "$$\n",
    "\n",
    "*Computation of $v^T Du$*\n",
    "We have\n",
    "$$\n",
    "v^T \\cdot Du=\\frac{1}{\\alpha}(\\langle v,h(p) \\rangle q^T-\n",
    "v^T \\odot h(p)^T).\n",
    "$$\n"
   ],
   "id": "6f432a40b0545b15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from math import log\n",
    "import time\n",
    "import numpy as np\n",
    "import torch"
   ],
   "id": "c1375ff3ec4a1921"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LogSoftTopK(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def _solve(s, t, a, b, e):\n",
    "        z = torch.abs(e) + torch.sqrt(e**2 + a * b * torch.exp(s - t))\n",
    "        ab = torch.where(e > 0, a, b)\n",
    "\n",
    "        return torch.where(\n",
    "            e > 0, t + torch.log(z) - torch.log(ab), s - torch.log(z) + torch.log(ab)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, r, k, alpha, descending=False):\n",
    "        # Sprawdzenie wymiarów\n",
    "        assert r.shape[0] == k.shape[0], \"k must have same batch size as r\"\n",
    "\n",
    "        batch_size, num_dim = r.shape\n",
    "        x = torch.empty_like(r, requires_grad=False)\n",
    "\n",
    "        def finding_b():\n",
    "            scaled = torch.sort(r, dim=1)[0]\n",
    "            scaled.div_(alpha)\n",
    "\n",
    "            eB = torch.logcumsumexp(scaled, dim=1)\n",
    "            eB.sub_(scaled).exp_()\n",
    "\n",
    "            torch.neg(scaled, out=x)\n",
    "            eA = torch.flip(x, dims=(1,))\n",
    "            torch.logcumsumexp(eA, dim=1, out=x)\n",
    "            idx = torch.arange(start=num_dim - 1, end=-1, step=-1, device=x.device)\n",
    "            torch.index_select(x, 1, idx, out=eA)\n",
    "            eA.add_(scaled).exp_()\n",
    "\n",
    "            row = torch.arange(1, 2 * num_dim + 1, 2, device=r.device)\n",
    "\n",
    "            torch.add(torch.add(eA, eB, alpha=-1, out=x), row.view(1, -1), out=x)\n",
    "\n",
    "            w = (k if descending else num_dim - k).unsqueeze(1)\n",
    "            i = torch.searchsorted(x, 2 * w)\n",
    "            m = torch.clamp(i - 1, 0, num_dim - 1)\n",
    "            n = torch.clamp(i, 0, num_dim - 1)\n",
    "\n",
    "            b = LogSoftTopK._solve(\n",
    "                scaled.gather(1, m),\n",
    "                scaled.gather(1, n),\n",
    "                torch.where(i < num_dim, eA.gather(1, n), 0),\n",
    "                torch.where(i > 0, eB.gather(1, m), 0),\n",
    "                w - i,\n",
    "            )\n",
    "            return b\n",
    "\n",
    "        b = finding_b()\n",
    "\n",
    "        sign = -1 if descending else 1\n",
    "\n",
    "        torch.div(r, alpha * sign, out=x)\n",
    "        x.sub_(sign * b)\n",
    "\n",
    "        sign_x = x > 0\n",
    "        qx = torch.relu(x).neg_().exp_().mul_(-0.5).add_(1)\n",
    "\n",
    "        ctx.save_for_backward(x, qx, r)\n",
    "        ctx.alpha = alpha\n",
    "        ctx.sign = sign\n",
    "\n",
    "        log_p = torch.where(sign_x, torch.log(qx), x.sub(log(2)))\n",
    "        return log_p\n",
    "\n",
    "    # @staticmethod\n",
    "    # def backward(ctx, grad_output):\n",
    "    #     x, qx, r = ctx.saved_tensors\n",
    "    #     alpha = ctx.alpha\n",
    "    #     sign = ctx.sign\n",
    "\n",
    "    #     w = 1 / qx - 1\n",
    "    #     wgrad = w * grad_output\n",
    "    #     wsum = wgrad.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #     q = torch.softmax(-torch.abs(x), dim=1)\n",
    "    #     R = 0.5 * torch.exp(-torch.abs(x)).sum(dim=1)\n",
    "\n",
    "    #     grad_k = abs(sign) / R * wsum.squeeze(1)\n",
    "    #     grad_r = -sign / alpha * (wsum * q - wgrad)\n",
    "    #     grad_alpha = -1 /alpha *(grad_r * r).sum()\n",
    "\n",
    "    #     return grad_r, grad_k, grad_alpha, None\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, qx, r = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "        sign = ctx.sign\n",
    "\n",
    "        x.abs_().neg_()\n",
    "        grad_r = torch.softmax(x, dim=1)\n",
    "        x.exp_()\n",
    "        grad_k = torch.sum(x, dim=1).mul_(0.5)\n",
    "\n",
    "        qx.reciprocal_().sub_(1)\n",
    "        qx.mul_(grad_output)  # wgrad\n",
    "\n",
    "        wsum = qx.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Gradients\n",
    "        grad_k.reciprocal_().mul_(wsum.squeeze(1)).mul_(abs(sign))\n",
    "        grad_r.mul_(wsum).sub_(qx).mul_(-sign / alpha)\n",
    "\n",
    "        x.copy_(r).mul_(grad_r)\n",
    "        grad_alpha = torch.sum(x).div_(-alpha)\n",
    "\n",
    "        return grad_r, grad_k, grad_alpha, None\n",
    "\n",
    "\n",
    "def log_soft_top_k(r, k, alpha, descending=False):\n",
    "    return LogSoftTopK.apply(r, k, alpha, descending)"
   ],
   "id": "302f938f2d0f08a3"
  },
  {
   "cell_type": "markdown",
   "id": "d0826aed-73fc-4aa8-bd8f-01a47364a60e",
   "metadata": {},
   "source": "## Test"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def numerical_vjp(x, k, alpha, descending, v, h=1e-5):\n",
    "    grad_approx = torch.zeros_like(x)\n",
    "    for i in range(x.numel()):\n",
    "        e = torch.zeros_like(x).view(-1)\n",
    "        e[i] = h  # Perturb one dimension at a time\n",
    "        e = e.view_as(x)  # Reshape back to original shape\n",
    "\n",
    "        grad_approx.view(-1)[i] = torch.dot(\n",
    "            v.flatten(),\n",
    "            (\n",
    "                log_soft_top_k(x + e, k, alpha, descending)\n",
    "                - log_soft_top_k(x - e, k, alpha, descending)\n",
    "            ).flatten(),\n",
    "        ) / (2 * h)\n",
    "    return grad_approx\n",
    "\n",
    "\n",
    "def check_value(x, v, text):\n",
    "    assert x.shape == v.shape, f\"Shape mismatch: {x.shape} vs {v.shape}\"\n",
    "\n",
    "    def fun():\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return torch.allclose, torch.linalg.norm\n",
    "        else:\n",
    "            return np.allclose, np.linalg.norm\n",
    "\n",
    "    function, dist = fun()\n",
    "    check = None\n",
    "    for tol_exp in range(-15, 0):\n",
    "        if function(x, v, rtol=1e-05, atol=10**tol_exp):\n",
    "            check = f\"Error within atol=1e{tol_exp}\"\n",
    "            break\n",
    "    if check:\n",
    "        print(f\"✅ - {text} ({check})\")\n",
    "    else:\n",
    "        print(f\"❌ - {text} [dist: {dist(x - v):.4f}]\")\n",
    "        print(f\"Expected: {v}\")\n",
    "        print(f\"Got: {x}\")\n",
    "\n",
    "\n",
    "def print_time_stats(times, name):\n",
    "    if not times:\n",
    "        return\n",
    "    avg = sum(times) / len(times)\n",
    "    min_t = min(times)\n",
    "    max_t = max(times)\n",
    "    print(f\"\\n{name} time stats (seconds):\")\n",
    "    print(f\"\\033[0;1;35m  Average: {avg:.4f}\\033[0m\")\n",
    "    print(f\"  Min:     {min_t:.4f}\")\n",
    "    print(f\"  Max:     {max_t:.4f}\")\n",
    "    print(f\"  All times: {[f'{t:.4f}' for t in times]}\")"
   ],
   "id": "7f2561ca30b18483"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# ==============  Parameters  =================\n",
    "use_gpu = False\n",
    "use_gpu = True\n",
    "\n",
    "descending = False\n",
    "descending = True\n",
    "\n",
    "h = 1e-5\n",
    "\n",
    "bs = 3\n",
    "n = 500\n",
    "# =============================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "print(f\"{device=}\\n\")\n",
    "\n",
    "factory_kwargs = {\"device\": device, \"requires_grad\": True}\n",
    "\n",
    "for i in range(3):\n",
    "    alpha = torch.tensor(np.random.rand(), **factory_kwargs)\n",
    "\n",
    "    r = torch.randn(bs, n, **factory_kwargs)\n",
    "    k = torch.tensor(np.random.rand(bs) * n, **{**factory_kwargs, \"dtype\": r.dtype})\n",
    "\n",
    "    print(f\"bs={bs}, n={n}, alpha={alpha.item()}\")\n",
    "    assert (\n",
    "        alpha.dtype == k.dtype == r.dtype\n",
    "    ), f\"You have different types of tensors: {alpha.dtype=}, {k.dtype=}, {r.dtype=}\"\n",
    "\n",
    "    # For Backward computation\n",
    "    v = torch.randn_like(r)\n",
    "\n",
    "    # Forward pass\n",
    "    start_forward = time.perf_counter()\n",
    "    prob = log_soft_top_k(r, k, alpha, descending)\n",
    "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "    forward_time = time.perf_counter() - start_forward\n",
    "    print(f\"\\033[0;32mForward pass time: {forward_time:.4g} s\\033[0m\")\n",
    "\n",
    "    # Test sum\n",
    "    test_sum = torch.logsumexp(prob, dim=-1).exp()\n",
    "    check_value(test_sum, k, \"test sum\")\n",
    "\n",
    "    # ======================================================\n",
    "    print(\"=\" * 10, \"Gradients\", \"=\" * 10, sep=\"   \")\n",
    "\n",
    "    # Backward pass\n",
    "    start_backward = time.perf_counter()\n",
    "    r.grad = None  # Clear gradients\n",
    "    k.grad = None\n",
    "    alpha.grad = None\n",
    "    prob.backward(v)\n",
    "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "    backward_time = time.perf_counter() - start_backward\n",
    "    print(f\"\\033[0;34mBackward pass time: {backward_time:.4g} s\\033[0m\")\n",
    "    print(f\"\\033[0;33mTotal time: {forward_time + backward_time:.4g} s\\033[0m\")\n",
    "\n",
    "    # try:\n",
    "    #     torch.autograd.gradcheck(\n",
    "    #         lambda r: log_soft_top_k(r, k, alpha, descending),\n",
    "    #         r,\n",
    "    #         eps=1e-6,\n",
    "    #         atol=1e-5,\n",
    "    #         rtol=1e-3\n",
    "    #     )\n",
    "    #     print(\"✅ r gradient passed\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"❌ r gradient failed: {str(e)}\")\n",
    "\n",
    "    # try:\n",
    "    #     torch.autograd.gradcheck(\n",
    "    #         lambda k: log_soft_top_k(r, k, alpha, descending).sum(),\n",
    "    #         k,\n",
    "    #         eps=1e-6,\n",
    "    #         atol=1e-5,\n",
    "    #         rtol=1e-3\n",
    "    #     )\n",
    "    #     print(\"✅ k gradient passed\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"❌ k gradient failed: {str(e)}\")\n",
    "\n",
    "    numerical_derivative = numerical_vjp(r, k, alpha, descending, v, h)\n",
    "    check_value(r.grad, numerical_derivative, \"grad r\")\n",
    "\n",
    "    numerical_k_grad = (\n",
    "        torch.mul(\n",
    "            v,\n",
    "            log_soft_top_k(r, k + h, alpha, descending)\n",
    "            - log_soft_top_k(r, k - h, alpha, descending),\n",
    "        )\n",
    "        / (2 * h)\n",
    "    ).sum(1)\n",
    "    check_value(k.grad, numerical_k_grad, \"grad k\")\n",
    "\n",
    "    numerical_alpha_grad = torch.mul(\n",
    "        v,\n",
    "        log_soft_top_k(r, k, alpha + h, descending)\n",
    "        - log_soft_top_k(r, k, alpha - h, descending),\n",
    "    ) / (2 * h)\n",
    "    check_value(alpha.grad, numerical_alpha_grad.sum(), \"grad alpha\")\n",
    "    print()"
   ],
   "id": "334ca76f9baa11e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d0a37d44baa6145a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d8d1e55541c0c354"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "435568ad2b46dfc4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
